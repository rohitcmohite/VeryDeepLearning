{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise4_VDL_full.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUhHJZz3thDT"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "In this exercise, you will understand and implement the below concepts :\n",
        "\n",
        "*   Reading a text dataset.\n",
        "*   Pre-processing the text such as Stopwords Removal etc.\n",
        "*   Word embedding \n",
        "    1.   Vocabulary Indexing\n",
        "    2.   Glove embedding\n",
        "*   Implementing Dataloaders\n",
        "*   A Neural Network\n",
        "    1.   RNNs\n",
        "    2.   LSTMs\n",
        "*   Evaluation of the network\n",
        "\n",
        "\n",
        "The task here is to develop a simple sentiment analysis model for a dataset by understanding if the review is positive or negative for each statement.\n",
        "\n",
        "# Task 1 : A simple Sentiment Analysis model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4TbwiCgvpTx"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUUzk0UrFVvm",
        "outputId": "27ecd67a-de1b-4154-bb4c-04a56c9bd1bf"
      },
      "source": [
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne28nPIwFZDO",
        "outputId": "74b01886-10d6-43a2-b409-83135e9baa65"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h70smCpfIAyP"
      },
      "source": [
        "import collections\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from string import punctuation\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv6les9-Fc4v"
      },
      "source": [
        "# Read the input CSV file\r\n",
        "\r\n",
        "Write a function to read the text data with the review text as features and the sentiment (1 for positive and 0 for negative) as labels.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4MBGif2FcCo"
      },
      "source": [
        "def read_file(file_name):\r\n",
        "  \"\"\"\r\n",
        "      file_name: The name of the CSV Input data file \"SJ_Unsupervised_NLP_data.txt\"\r\n",
        "        \r\n",
        "      return the features and labels\r\n",
        "  \"\"\"\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  with open(file_name, 'r') as f:\r\n",
        "    data = f.read()\r\n",
        "\r\n",
        "  # Splitting the data to make list of reviews and labels\r\n",
        "  data = [i.split('\\t') for i in data.split('\\n')]\r\n",
        "\r\n",
        "  # Removing last blank element\r\n",
        "  if len(data[-1])==1:\r\n",
        "    data.pop(-1)\r\n",
        "\r\n",
        "  train_x = [i[0] for i in data]\r\n",
        "  train_y1 = [i[1] for i in data]\r\n",
        "\r\n",
        "  # Changing datatype of list from string to int\r\n",
        "\r\n",
        "  train_y = [int(i) for i in train_y1]\r\n",
        "  \r\n",
        "  return train_x, train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9pbKGXfF9iN"
      },
      "source": [
        "# Pre-process the sentences\r\n",
        "\r\n",
        "*   For example, remove the stopwords, special characters and so on. \r\n",
        "*   The most important step here is to tokenize the sentences. \r\n",
        "\r\n",
        "Expect that the accuracy would be more if your dataset is well pre-processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrqIFOYfF7V3"
      },
      "source": [
        "def preprocess_text(inputtext, tokenize=True):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "      text: The review text.\r\n",
        "        \r\n",
        "      return: Tokenized pre-processed text if Tokenize=true, else just the pre-processed text.\r\n",
        "    \"\"\"\r\n",
        "    ######################################\r\n",
        "    ######### YOUR CODE HERE #############\r\n",
        "    ######################################\r\n",
        "\r\n",
        "    text = np.array([])\r\n",
        "    for line in inputtext:\r\n",
        "      line = line.lower()  #  Converted all to lower-case\r\n",
        "      unpunctuated = ''.join([ch for ch in line if ch not in punctuation])  # All punctuations removed\r\n",
        "      stopwords_removed = ' '.join([word for word in unpunctuated.split() if word not in stop_words])  # All stopwords removed\r\n",
        "      text = np.append(text, stopwords_removed)\r\n",
        "\r\n",
        "    if tokenize:\r\n",
        "      text1 = []\r\n",
        "      for line in text:\r\n",
        "        tokanized_line = word_tokenize(line)\r\n",
        "        text1.append(tokanized_line)\r\n",
        "        \r\n",
        "      text = text1\r\n",
        "\r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ew5GA8IHbU7"
      },
      "source": [
        "# Building the vocabulary\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "*   Count the number of occurances of all the words from all of the reviews by using a Counter function.\r\n",
        "*   Sort the words in descending order.\r\n",
        "*   Print the 10 more frequently occuring words (just to cross verify with the dataset).\r\n",
        "\r\n",
        " \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9sPKl27LGZV"
      },
      "source": [
        "def count_words(all_reviews):\r\n",
        "  \"\"\"\r\n",
        "      all_reviews: all the review sentences which are tokenized.\r\n",
        "        \r\n",
        "      return: The list of words based on the decreasing order of their occurances. Also return the length of the list. Sample given below.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  text1=[]\r\n",
        "  for sentence in all_reviews:\r\n",
        "    for word in sentence:\r\n",
        "      text1.append(word)\r\n",
        "\r\n",
        "  counts = Counter(text1)    # get the count of occurences of individual words\r\n",
        "\r\n",
        "  words_list = sorted(counts, key=counts.get, reverse=True)     # Sort the words in descending order of their occurences\r\n",
        "\r\n",
        "  print('The 10 more frequently occuring words are: ', words_list[:10])\r\n",
        "\r\n",
        "  return words_list, len(words_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSntjwJvL-Lo"
      },
      "source": [
        "# Embedding the words\r\n",
        "\r\n",
        "*   Encode the reviews in number format based on the index of the word in the vocabulary. Build a function word2idx_embedding from scratch such that for each review containing list of tokens(words), a numbered vector of it's corresponding positions in the vocabulary is returned. \r\n",
        "\r\n",
        "Note : Word-to-index is a variant of the one-hot-encoding where instead of filling 0's and 1's throughout the vocabulary for each word present in the review, we replace the review words by it's corresponding index value from the vocabulary. This drastically reduces the run-time. \r\n",
        "\r\n",
        "Make sure you add +1 since the index of the vocab starts from 0, and we want the word positions from 1.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdfQ2D-tM-gq"
      },
      "source": [
        "def word2idx_embedding(words_list, reviews_in_words):\r\n",
        "    \"\"\"\r\n",
        "        words_list: list of all words in the vocabulary sorted in descending order\r\n",
        "        reviews_in_words: All the reviews tokenized as words\r\n",
        "        \r\n",
        "        return a list of vocab_to_int or the word2idx (the dictionary of words and it's integer positions) \r\n",
        "        and also the list of vectors which are encoded by word2idx for all reviews\r\n",
        "    \"\"\"\r\n",
        "    ######################################\r\n",
        "    ######### YOUR CODE HERE #############\r\n",
        "    ######################################\r\n",
        "    #words_list = tuple(words_list)\r\n",
        "\r\n",
        "    vocab_to_int = {value: index for index,value in enumerate(words_list, start=1)}\r\n",
        "\r\n",
        "    emdedded_vector_list = []\r\n",
        "    for review in reviews_in_words:\r\n",
        "        emdedded_vector_list.append([vocab_to_int[word] for word in review])\r\n",
        "\r\n",
        "    return vocab_to_int, emdedded_vector_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tOGZXjif-vA"
      },
      "source": [
        "*   Pad the embedded vector of each review to bring up a unified vector length defined by the sequence length.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e24LNSc_gnkw"
      },
      "source": [
        "def pad_text(encoded_reviews):\r\n",
        "  \"\"\"\r\n",
        "        encoded_reviews: list of embedded vectors which are yet to be padded.\r\n",
        "        \r\n",
        "        return the numpy array of vectors\r\n",
        "    \"\"\"\r\n",
        "  seq_length = 20\r\n",
        "\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  padded_reviews = np.zeros((len(encoded_reviews), seq_length), dtype=int)\r\n",
        "\r\n",
        "  for id, rev in enumerate(encoded_reviews):\r\n",
        "      padded_reviews[id, :len(rev)] = np.array(rev)[:seq_length] \r\n",
        "\r\n",
        "  \r\n",
        "  return np.array(padded_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmoSVpwvk-n5"
      },
      "source": [
        "# Build dataloaders\r\n",
        "\r\n",
        "*   Split the train, validation and test as 80/10/10 respectively.\r\n",
        "*   Create the data loaders using the TensorDataset function with a batch size of 20.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYfW8iMMk9xf"
      },
      "source": [
        "def get_loaders(data, labels, batch_size):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "      return the train, validation and test set loaders\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  # Splitting the Data\r\n",
        "  data = np.array(data)\r\n",
        "  labels = np.array(labels)\r\n",
        "  \r\n",
        "  train_split = 0.8\r\n",
        "  split_idx = int( len(data)* train_split )\r\n",
        "\r\n",
        "  train_x, remaining_x = data[:split_idx], data[split_idx:]\r\n",
        "  train_y, remaining_y = labels[:split_idx], labels[split_idx:]\r\n",
        "\r\n",
        "  val_split = len(remaining_x)//2\r\n",
        "\r\n",
        "  val_x, test_x = remaining_x[:val_split], remaining_x[val_split:]\r\n",
        "  val_y, test_y = remaining_y[:val_split], remaining_y[val_split:]\r\n",
        "\r\n",
        "  \r\n",
        "  # Create Tensor Dataset\r\n",
        "  train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\r\n",
        "  val_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\r\n",
        "  test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\r\n",
        "\r\n",
        "  # Creating Dataloaders\r\n",
        "  tr_loader = DataLoader(train_data, batch_size= batch_size, shuffle=True)\r\n",
        "  v_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\r\n",
        "  t_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\r\n",
        "\r\n",
        "  return tr_loader, v_loader, t_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbmiJa36mSzV"
      },
      "source": [
        "# Build your model\r\n",
        "\r\n",
        "*   Build a simple RNN architecture model based on the below visualization.\r\n",
        "![RNN.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4REMRXhpZgAATU0AKgAAAAgABAE7AAIAAAAWAAAISodpAAQAAAABAAAIYJydAAEAAAAsAAAQ2OocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEtpbm5hbCwgTmF2ZWVuIFZpc2hudQAABZADAAIAAAAUAAAQrpAEAAIAAAAUAAAQwpKRAAIAAAADNzkAAJKSAAIAAAADNzkAAOocAAcAAAgMAAAIogAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjE6MDE6MTIgMTM6NDQ6MjIAMjAyMTowMToxMiAxMzo0NDoyMgAAAEsAaQBuAG4AYQBsACwAIABOAGEAdgBlAGUAbgAgAFYAaQBzAGgAbgB1AAAA/+ELKGh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iLz48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjEtMDEtMTJUMTM6NDQ6MjIuNzg5PC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyI+PGRjOmNyZWF0b3I+PHJkZjpTZXEgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj48cmRmOmxpPktpbm5hbCwgTmF2ZWVuIFZpc2hudTwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCAGtAgoDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6RooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACqd7q1np0iR3cjK8gJVViZyQOv3QfUVcrE1P/AJGK0/69pf8A0JK5sTUnTp80N7rfzfyNaUVOVmTf8JJpn/PSf/wFl/8AiaP+Ek0z/npP/wCAsv8A8TTaK5fbYjuvuf8A8kdHsafn9/8AwB3/AAkmmf8APSf/AMBZf/iaP+Ek0z/npP8A+Asv/wATTaKPbYjuvuf/AMkHsafn9/8AwB3/AAkmmf8APSf/AMBZf/iaP+Ek0z/npP8A+Asv/wATTaKPbYjuvuf/AMkHsafn9/8AwB3/AAkmmf8APSf/AMBZf/iaP+Ek0z/npP8A+Asv/wATTaKPbYjuvuf/AMkHsafn9/8AwB3/AAkmmf8APSf/AMBZf/iaP+Ek0z/npP8A+Asv/wATTaKPbYjuvuf/AMkHsafn9/8AwB3/AAkmmf8APSf/AMBZf/iaP+El0wdZZv8AwFl/+JptMm/1L/7p/lQ62IS3j9z/APkh+xp+f3/8A1oJ47m3jngbdHIoZGxjIPTrUlUNC/5F+w/690/9BFX676UnOnGT6pHHJWk0FFFFaEhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWJqf/ACMVp/17S/8AoSVt1ian/wAjFaf9e0v/AKElceM/hfNfmjfD/H9/5ElFFFYHUFFFFABRRRQAUUUUAFFFFABTJv8AUv8A7p/lT6ZN/qX/AN0/yqZbMa3Lehf8i/Yf9e6f+gir9UNC/wCRfsP+vdP/AEEVfruw/wDBh6L8jgqfG/UKKKK3ICiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAorhPiJ4p1Tw7rPheDTJkii1HURBc7ow25NjHqenQc111trWl3l01tZ6lZzzr96KKdWYfUA5oAu0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFYmp/8jFaf9e0v/oSVt1ian/yMVp/17S/+hJXHjP4XzX5o3w/x/f8AkSUUUVgdQUUVyWqfE/wlo2s3GlX+pOl7akCaJLWWTZkZHKqR0NCTewm0tzraKxPD/jHQfFHmDQ9RjuXj+/HtKOv/AAFgD+lLovi/Q/EWoX1jo9+txc2EhjuY9jKUYHB6gZ57iizC6Nqisy08R6XfajqNja3W+50zH2tNjDy85xyRg9D0qLw34s0XxdZSXfh6+W8hjfY7BGXafTDAGizC6Niis7Rtf03xBFcyaTc/aFtZzbynYy7XABI5A9RWjQMKZN/qX/3T/Kn0yb/Uv/un+VTLZjW5b0L/AJF+w/690/8AQRV+qGhf8i/Yf9e6f+gir9d2H/gw9F+RwVPjfqFFFFbkBRRRQAUUUUAFFFFABRRRQAUUVwnjrxTqeg+LvCljYTJHbaldmK6DIDuUDPU9KAO7oqla61pd9cNBY6lZ3My/ejhnV2H1AOau0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHkXxxsv7S1LwZZmRo1m1cIzIcHaUbIz7jIqDx/wCGdK8Ia/4P1Lw9bfYrganHDK0bt++QsoIbJ57/AJ12Hjvwhf8AibWPDd1YSQIml34uZhKxBZdrDC4Byefan+P/AAnf+KZNCbT5IEGn36XMvnMRlQQSBgHnigDe1fxBYaElmdRkKfbJ1t4gBnLkEgfkprSDBvukH6GvKvjH4d0vUb7wrNfWcc0suqpbuzD70ZjkJX6ZUflXoWheG9J8M2kltodjFZwyPvZIxgFsYzQBqVT1PVrHRrGS81O5jtoI1LM8jYrBPjWceIP7M/4RjWSnm+X9s8uPyf8Aezvzj8K09a8OaRrEsd5qmnwXU9qjeS8qZMf0oAXw14m07xboqarozvJayMyqzrtJwcHj8K1q4H4NAL4CIAwBfXIA/wC2rV31ABTfMT++v50SxJPC8Uqh43UqynoQeorl2+GHghmLN4Y00knJPkjmgDqPMT++v50eYn99fzrlv+FXeB/+hX03/vwKP+FXeB/+hX03/vwKAOp8xP76/nR5if31/OuW/wCFXeB/+hX03/vwKP8AhV3gf/oV9N/78CgDqfMT++v50eYn99fzrlv+FXeB/wDoV9N/78Cj/hV3gf8A6FfTf+/AoAwdf+M2l+EPHL6B4otJLOCRVktb5W3LIp4yR25BGc10X9q2GtanYXuk3cN5bSWspSWFwyn5k7ivL/EP7PUXijx7JdeXZaHoNuixwxWSEyT9yWHAHJxnJ6V3+i+DdG8E3dnp2gWxhh+zyFmY7mc7l5J71x4z+F81+aN8P8f3/kdDRRRWB1BXhVv4q0fwt8bvHE2u2txcRyC32mG187bhBkn0617rXnnhfw3qNr8WvGep6hYldP1BYBbTPgrLtXDYGc/nVxaV7kSvpYxfBsZ8WfFE+MND02TTtEWzMKyPH5f2piV52+nB5rkvDUkvhnxPqPi+LP2VNensdQHby3kIVz9C2fwr0fwloms+C/Gl7pFrZyT+GLwme2lVhi0fPKEE5wc9vSofB/gu6l8PeLdK8Q2bQRapqNw8W/B3I5O1xg++avmWpHKyt4OdZfiF8Q5EOVZYSD6grJXG/CWd/CB0PUnJGmeIN9rNn7sc6k7D+IXH411Hwo8L+JdFufFJ8R2TxvcRxx28pYHzwokGeD7jrVjw/wCBL+4+BqaDqlq1pqkO+aBWILRyq+5DkHuQPzptpXXoCTdmXPgx/wAgzxH/ANhub/0FK9Irzr4L6Hreh+F9Qj8S2TWl5PftLtZgdw2IN3B7kGvRayn8RpH4Qpk3+pf/AHT/ACp9Mm/1L/7p/lWctmWtyzoUiDw/YZZf+PdO/wDsir/mJ/fX864zTfh14RvtLtru70CylnniWSSRo8lmIySatf8ACr/BX/QuWP8A36Fd2H/gw9F+RwVPjfqdT5if31/OjzE/vr+dct/wq/wV/wBC5Y/9+hR/wq/wV/0Llj/36FbkHU+Yn99fzo8xP76/nXLf8Kv8Ff8AQuWP/foUrfDLwYxBbw7YnAx/qxQB1HmJ/fX86PMT++v51y3/AArDwZ/0L1l/37o/4Vh4N/6AFp/3xQB0OoXb22m3M9pELmeOJnjhDYMjAZC59zxXDeDfjT4X8XTfY2uBp2pK2xrW5cDLdMKf4ulXtS+GnhmLS7qTTvDtnLdrExgjZcBnx8oJ9M15v4M/ZqtYL46p4xuhLMzl1s7UkIncfNwT9MUAe+9aKjt4I7W3jggUJHGoVFHYDpUlABXkPxm04at4w8EWMkjxxzX5WQocEqR8wz2yMivXq4zxl4Rv/EHirwzqVlJAkOk3RmnEjEMy4x8uAcn8qAOQ8beHNL8IeNvBl/4dthYyyXxgm8tj++QgcNk816fq/iDT9EnsYdQlKPfz/Z4ABnL7S38gawPHXhK/8S6r4dubCSBE0y9+0TCViCVxj5cA5Ncx8XfDmk6l4s8Hy31lFM9zqXkSlh99PKc7T7ZAoA9YDBvukH6GlrN0Pw9pXhqxaz0Szjs7dnMjRxjALYAz+grSoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMDxf4StfF+lw2l1cTWslvOtxBcQEB4nAIBGeOjHrU/h7Q7rRYZUvNZu9WeRs+ZdKgK+wCgCtiigArC8R+H77XPKFj4gvdIVAQ4tY428zPrvU/pW7RQBxXg/4eT+D5kW38T6jd2as7m0njiCMzHJOQoPU+tdrRRQAUUUUAFFFFABRRRQAUUUUAFYmp/wDIxWn/AF7S/wDoSVt1ian/AMjFaf8AXtL/AOhJXHjP4XzX5o3w/wAf3/kSUUUVgdQUUUUAFFFFABRRRQAUUUUAFMm/1L/7p/lT6ZN/qX/3T/Kplsxrct6F/wAi/Yf9e6f+gir9UNC/5F+w/wCvdP8A0EVfruw/8GHovyOCp8b9QooorcgKKKKACiiigAooooAKKKKACiiigArnvF/g+18X2dpHPdT2c9lOLi2ubcjfE4BGeeDwSOfWuhooAyPD+i3Oi2skd5q93qru27zbkKCOOgCgCteiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsTU/8AkYrT/r2l/wDQkrbrE1P/AJGK0/69pf8A0JK48Z/C+a/NG+H+P7/yJKKKKwOoKKKKACiiigAooooAKKKKACmTf6l/90/yp9Mm/wBS/wDun+VTLZjW5b0L/kX7D/r3T/0EVfqhoX/Iv2H/AF7p/wCgir9d2H/gw9F+RwVPjfqFFFFbkBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWJqf8AyMVp/wBe0v8A6ElbdZ+oaUb66iuEupLeSJGQFFU5BIPcH0rlxUJTp2iru6/Bm1GSjO7IKKT+w7j/AKC1x/37j/8AiaP7DuP+gtcf9+4//ia5eWr/ACP71/mdPtId/wAxaKT+w7j/AKC1x/37j/8AiaP7DuP+gtcf9+4//iaOWr/I/vX+Ye0h3/MWik/sO4/6C1x/37j/APia474leJ4/h14fi1G71K4nkmmWOKEJGC/I3Hp2HNHLV/kf3r/MPaQ7/mdlRVTS7Q6vpVtqFnrU0kFxGJEZUjIIP4Va/sO4/wCgtcf9+4//AImjlq/yP71/mHtId/zFopP7DuP+gtcf9+4//iaP7DuP+gtcf9+4/wD4mjlq/wAj+9f5h7SHf8xaZN/qX/3T/Knf2Hcf9Ba4/wC/cf8A8TSHQrhlIOrXGCMH93H/APE0nGs18D+9f5h7Sn3/ADLWhf8AIv2H/Xun/oIq/UNnbLZWMNsjFlhQICepAGKmr0KMXCnGL3SRxTacm0FFFFakhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXg/xu+HmteLtXOqX9/Fa6RZmK3tIkBdnLsAzMOMYLH8BXvFcp8SP+RPP/X3b/8Ao1aAKHwp8Na54Q8NPomt3EV3bwSbrOdGOTGR90jtjHqetd1UcH/HtF/uD+VSUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXKfEj/kTz/192/8A6NWurrlPiR/yJ5/6+7f/ANGrQB08H/HtF/uD+VSVHB/x7Rf7g/lUlABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABTJZUgheWVgscalmY9AByTT6zfEcEt14V1WC3BMstlMiAd2KED9aAOGl+K39paVf3en+H9SbSIt8f9qL93jgsFxuIBzyPSn/DXxLHp/wAE7PXdfvZZI4oi8s0zl2bkY5PJOTWL4K8W6HYfA1LG6uY1vLSCaCa0I+cyb2+XHqcj86wo9PutS/ZVtRZCQmIpM6x/eKBhnHuOv4UAd0fiz9mtoNQ1fwzqWn6POyhdQlwVAY4VmUDKg5HWtjxb8QrDwlc6PHc2s90urMyxNAMkYAI46nO7tXm95Y+G9T8E2/8AanxB1q5sruKOM2YlRiScDZt29jxW94zsobXxp8MrWMtJFFdSBDJySNq4zQB0en/EPzPFdroGt6JdaPdXyM9o0zq6zYGSMr0OATg+lW/EnjdND1q30aw0y41bVJ4vOFtAQu2PJG4seByD+Vcz8ROPir4EI6/apBn/ALZSVY8baf4e1PxlbiXW7rQfEMFrmC7hYKGjLHg5HODnjjrQB03hnxZ/b891aXWl3el3trjzILhex6EMOG6dqx7z4lb9YvNP8N6Be66bBtl1NbsFSN8ZK5bhiPQVl+Atf1mfxfq3hy/1KDW7e1tlkj1SKIodx42Nycn/AArifBGlra6r4k0/VvF+qeH72LU5pTbxSqiyIzFlcZU5yCKAPaPCniuw8XaQb7T96GOQxTQyqQ8TjqpB+tbdcF8LdJ0WystTvtA1K91KO8uiZri7x88gHJGAPUV3tABRRRQAUUUUAFFFFABRRRQAUUUUAFFZuueIdJ8NWH23Xb6KyttwTzJc4yeg4qPQvFGi+JoppNB1CK9SEgSGMH5SenUexoA1qKikureFts08UbejOAakBDAFSCD0IoAWiikJCgliAB1JoAWioY7u2lfZFcRO3orgmpqACiiigAoopkk0cWPNkVNxwNzAZNAD6KKKACikZ1RSzsFUdSTgCmRXME5IhmjkI67HBxQBJXKfEj/kTz/192//AKNWurrF8V6NNr2hmyt3VH8+KTLdMK4Y/wAqANaD/j2i/wBwfyqSmxrsiRT1VQKdQAUUyWaOFN00ixr6uwAp4IIyORQAUUVHNcQ24BnmjiB6b2Az+dAElFQx3trM+yG5hkb+6sgJqagAooooAKKKKACiiigAooooAKK5zWviD4V8O6hJZa3rVvZ3Mahnjk3ZUEZHat+3uIrq2juLdxJFKodHHRgeQaAJKKKKACiiigAooooAKKKKACiignAyeKAMj/hE/D39pnUf7E0/7axybn7MvmH/AIFjNXrLTbLTbFbPT7SG2tVGFhiQKgH0HFWQcjIpAQehBoAx7fwf4btNR+32uhadDebt3nx2yB8+u4DNX7nTLG9ura5u7OCee0YtbySRhmiJ6lSeh+lWQ6k4DDPpmlJA68UAVbnS7C9u7e6u7OCe4tiWglkjDNESMZUnpwTUOq6BpGuxrHrWmWl+i/dW5hWQD8xV8MrfdYH6GszWPEenaFPYQ6hKUk1C5W2gVRnLscDPoOetAFjTdI07Rrb7PpNjb2UOc+XbxBF/IVW1TwvoOuTLNrOjWN/InCvc26yEfQkVqBlb7pB+hoLqv3mA+poAjtraCzt0gtYkhhjGEjjXCqPYVLRnIzSb1AyWGPrQAtFAORkc0UAFFFFABRRRQAUUUUAFFFFAHmvxtUN4Z0pWAIOq2+Qe/wA4rv1jh0/T5JbeFIwsZchFxnAzXA/Gz/kW9J/7Ctv/AOhivRDGJrQxP9102n6EUAeCaIvhXXbWPXPiI0l5feINVls7AOWYQgSFEVcfd6deK7z4RahdnT9Y0O9uJLk6PftBFLI25jGQCoJPXHNYeiJq/wAP1vNFvPCF1rtvFeS3Om3NrB5gUOxcAnB2kE9a6r4Y+G9Q0PRr281xFj1LVbtrqaNT/qwQAF+oxQB21Yfi3w/L4m0JtNi1K405JHBlktzhmTuue2apPruvr8R00ddGc6IYN51Hy2wHxnbu6dam8bN4ji0NbjwiI5LyCUO8Dj/XIOqg9jQB4/q9l4Yg8VaTpfw8huNN1yG9Xz7py0CtGM7wS+N5PHTNfQdeN+K59b8fWdnpdh4KvtLuxdRyyX93EYxAFOSVbAya7H4h+Kr3wR4HS602KO81NnitreKYEiV2YL0BB75oA7OiqmlPeSaTbPqYjF20YaYRAhQx9ASTVugBHDFGCHaxHBx0rxD4l+F9Q05fDup63r91qt02uQRqrARxRqVcnCDjPA5xmvb2O1ScE4HQV4t8R/EGr+KbPSINP8F+Ig1jqsV3IXsXAKKrA44/2hQB7VRWP4b1+TxBZyzy6RqGlmOTYI7+Exs3GcgHqKzdL17xBdeP9S0q90V4NIt4w1vfmNgJT6Z6GgDmPjPcTxjQILua4t9BnvgmpSQFh8m1sBiOQucZ7VzGq3HhTwd4x8NXfw+u0BurxLe+hs5C8UkbEfexxu69ea9S8cza1b6LHLoWl22rBZl+1WkyFmki77B/e+oNcKmjXfjPxVon2bwpL4b0fSrgXlwbiDynuJAQQoGBkcdfegD2AHKg+tFcX8R/F+o+FrPSoNAt4LnU9SvUtoop1JUKQSzYBB4xXYQeb9mj+07TNsHmbBhd2Oce2aAJKRmCKWchVUZJJ4Apaz9e0r+29Bu9N+0y2ouozGZYiAyg9cZoA8T+L95deNvCWr6hA8kWgaSdkBBIF3MOre6rwB7g17fo/wDyBbP/AK4r/KvHvHXwv8Q2nw3ubDSPEer6rHGm2PTlgiIcc8fLGD+teqeEdLvNI8M21pqV/cX86jJluFUMM/w4UAcfSgDaryD4zW0V54v8HpfaLc6vYRvcNPbwQmQNxHtB7Dv1rtvC+veINU8Qazaa1oz2FnaTbLSdo2UXC+oJ6/hTPF+q+JdFv7G80XSV1bThuW7giUmdemCnr34xQByHgq48EWPjSGyj8Hy+GNZkRjbfaoUUyjHIVkJHTPevW68oltdb+IHxB0LUptBu9F0zRnaZpL1CkszFSAqggcfN+ldB448X6vo3iTw9ofhy3tp7zVJyJvtCswiiGMvgEep/KgDt6KQZ2jd1xziloAKKK5aHXdff4lXOjSaM66JHbLJHqPlth3IGV3dO5/KgDqaKQ8KcdcVzHgnXde1y3v28R6O+lvBOUgVo2XzU5+bnr2oA6iis7xBeXun+Hb670q1N5eQwO8FuFJMjgEhcDk5NQ+FNR1LVvCthfa5Ytp+oTxbp7VlKmJs9MHkUAc58YLS3b4W69M0MZl+zj5yoz94d66Twr/yKGkf9ecX/AKAKwfi//wAkl17/AK9//ZhW94V/5FDSP+vOL/0AUAa1FFFABRRRQAUUUUAFFFFABVHWtMi1rQ7zTZ/uXULRE+mRjP4VeooA8b8KeOX8MfCfWrbVWP8AaXhuSS1VHPzS/wAUf5hgPwqK7TUvh7+z75sbyrq18yNcSqcuryMN2D+YH1rc8SfCRtd+IUetxX8cOmTvDJqNiUJNy8TEg56dCB+Fdr4n8N2ninwzdaNe7kinQAOnVCDkEfQigDw++0sReHIrjwn4R8TWviKNUlj1F/K/evwSXIkyQf612HjK8v8AXte8HeF76Sayt9SR5tQSN9rSbAvyZHuxrUg8H+OTYW2lXfiy3XT4Cq+dbwbbh41xgFsYzgYyK1/GPgl/EX9mXmnX7WOr6S5e0uiu4cgAhh3BwKAKun/DOw8P+IrTUvDV9NpcUYK3FmvzR3IIPXkYOcHPPSuV+MXhfSNS8ZeC5ry0SSS81SO2nJ/jj3D5f1NdRYeEfE154ks9U8V+IEmjscmK008NFG7EEZfu3Xp0q7498HXPiu202XTL5bHUdLulurWV03KGBB5HfoKANDQPCeieD7a6/sCxS1WbDyhP4ioOP5mvNvA/hyy+JM2u634v8y+kXUZra2iZztt40YqAB2PGc16L4csvE9u0p8UalZXgYAIlrCUA+ua5uLwF4h8O6tqc3gnWrW1s9TmNxLbXkJkEcjfeZCPXrg8UAVPhbfXYg8U+Hbq5kuodHu2ht5ZGyfLK8Ln2x+tc18KfAtn4q8M3L+JpJb2ytbyaCztmchI1DkE47nivS/BvguPwnot1btctd319K011dOOZHYY/IUfD/wAJz+DvD8unXNzHcvJdTTh41IADuWA59M0AdDp9hb6Zp8NlZp5cEC7I19BViiigAooooAKKKKACiiigAooooA5jx54NPjXRIbBNRbTpIbhLhJliEmCpyOCRV3w3pOr6TBMmta6dYZyPLY2wh8sDORgMc54/KtqigAooooAKKKKACsvV/Dun65d6fcajG0jafN58C7sLv7EjvjqK1KKACiiigAooooAKKKKACiiigDLvfDun6hr1hrF2jPdaerrb/N8q7sZOPXjr7mrd/qFpplqbnULiO3hDBTJIcDJOAPzqzXzn+0h4h1PV72DwroFvcTx2hSa9aFGIDsQUUkcf3SPc0AfRasGUMpyCMgjvS1wPwd8Vz+JvA8UWqI8Wqacfs91HKu18gDDEds/0rvqACiiigAooooAKzH8P2EniZNedGa+jt/s6MW+VUyTwPU56/StOigAooooAKKKKACiiigAooooA4jxt4F1nxhHeWkfit9P0y6jVGtFsRJjGMncXHUjPSut0uxGmaTaWKuZBbQrEHIxu2jGcfhVqigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACuQ+I0ESeFXlSNFke7ttzheT+9Xqa6+uU+JH/Inn/r7t/8A0atAHR2kEMUKvFEiM6LuKqAW471YqOD/AI9ov9wfyqSgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK5T4kf8ief+vu3/wDRq11dcp8SP+RPP/X3b/8Ao1aAOng/49ov9wfyqSo4P+PaL/cH8qkoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACq91qFnZFReXUNuW+6JZAufpmrFc7rSK/iexDqGH2WXgjP8SVdOKlKzJk+VXNL+3tI/wCgpZ/9/wBf8aP7e0j/AKCln/3/AF/xrO+zw/8APGP/AL5FH2eH/njH/wB8itvZRMvaSNH+3tI/6Cln/wB/1/xo/t7SP+gpZ/8Af9f8azvs8P8Azxj/AO+RR9nh/wCeMf8A3yKPZRD2kjR/t7SP+gpZ/wDf9f8AGuZ+IGq6feeFTFaXtvNJ9qgbZHKGOBKpJwPatX7PD/zxj/75FH2eH/njH/3yKPZRD2ki9Br2ki3jB1OzBCjP79fT60/+3tI/6Cln/wB/1/xrO+zw/wDPGP8A75FH2eH/AJ4x/wDfIo9lEPaSNH+3tI/6Cln/AN/1/wAaP7e0j/oKWf8A3/X/ABrO+zw/88Y/++RR9nh/54x/98ij2UQ9pI0f7e0j/oKWf/f9f8aP7e0j/oKWf/f9f8azvs8P/PGP/vkUya3h+zyfuY/un+EelP2UQ9pI6KORJo1kidXRhlWU5BHqDTqzfDv/ACLOm/8AXrH/AOgitKuaSs2jZO6uFFFFIYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVz2r/8jRZf9ekv/oSV0Nc9q/8AyNFl/wBekv8A6Ela0fjM6nwktFFFdJiFFFFABRRRQAUUUUAFFFFABUc//HvJ/un+VSVHP/x7yf7p/lQBe8O/8izpv/XrH/6CK0qzfDv/ACLOm/8AXrH/AOgitKuOfxM6I/CgoooqSgooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArntX/5Giy/69Jf/Qkroa57V/8AkaLL/r0l/wDQkrWj8ZnU+ElooorpMQooooAKKKKACiiigAooooAKjn/495P90/yqSo5/+PeT/dP8qAL3h3/kWdN/69Y//QRWlWb4d/5FnTf+vWP/ANBFaVcc/iZ0R+FBRRRUlBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXPav8A8jRZf9ekv/oSV0Nc9q//ACNFl/16S/8AoSVrR+MzqfCcA2u+KtU8W63YaZq+kafa6dMkSLdWhkd90asTnzF7t6Vr3njOHwzBa2viCSS+v5IGmZ7C3wjKpGWALHAGR3NcVNb+GoPiB4lk8XaDNeNLcRG3lOmSTgp5KA4ZUI6g1f1OKHUNXs59D0+4SwXR7mKNfsrx7DmPC7SBj6Vrcixuan45tdU8Ka3JoklxBc21i1xFI6Bdylcq6cnI5FWPD3juxvo9Ks7hLxJ7yBPLuZogI532jdtbPJz7VxkWm3w0S+T7FcBm8LRxBfKbl9ifL06+1aEVzLrv/CH6ba6ddwXGmywz3fm27IsSoF43EYJOOgJouwsjrdQ8c6bp99cW32a9uRaY+0zW8IaOD/eORiptR8Y6bp8VgyLcXsmoLvtobSMO8i4znBI4xXntxpNrpviHxDFrqeIS19dPPbLp1xOsVwr5+U+WdoPrux1rXnt18K674b1NdOvE0uGwNu0ao08luSCQGC5J64yM07sVkXvC/jBbg+J9S1K7lFhZXZCCYYMKYHy4+tQSeNn1Txz4atLFb6xhunkaSG5jCecnlOQepyMgVg/2ZqWu+GfGxt9PuIJLy9E0EMiFWkUY9fpWnJrDeI/iF4UurLS72K3tzL58s1s8YiYxP8pyB69envSux2R0cviFJfHtlpki6rZvsl2I8Si3ucAEndnJxn0q5q3jDT9H1qHSZYbqe9nhaaKK3iDFwpAIHI5+YVyOueLdOk+IWh3kcOpNb2K3Mdw40y4IUsFA/g55B6VrIrXvxc07UYYJjato0+2V4mUAmSEgHI4OAeDzwadxWNeHxro8vhy41p5JIba2Zo5klTEiODjaRnrmq1r470zUNSh0n7Pe217dQSSxxXEIU7Fx8x5PB3cfQ1xt3o+oXHhPxIsNnMzprcd2sewgyxpIrttz1yAatvrMWtfE/QJbW1njij066XzJoWjLH93kYYA8euMc0XYWR614d/5FnTf+vWP/ANBFaVZvh3/kWdN/69Y//QRWlXNP4mbR+FBRRRUlBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXPav/yNFl/16S/+hJXQ1z2r/wDI0WX/AF6S/wDoSVrR+MzqfCS0UUV0mIUUUUAFFFFABRRRQAUUUUAVdSsItU02eyuC6xzIUZo2KsMjqCO9YemeDoNJvX1G41G91O6SBoYpLtlPlIeoG1R1wOuTxXTVHP8A8e8n+6f5UW1AveHf+RZ03/r1j/8AQRWlWb4d/wCRZ03/AK9Y/wD0EVpVxz+JnRH4UFFFFSUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFc9q//I0WX/XpL/6EldDXPav/AMjRZf8AXpL/AOhJWtH4zOp8JLRRRXSYhRRRQAUUUUAFFFFABRRRQAVHP/x7yf7p/lUlRz/8e8n+6f5UAXvDv/Is6b/16x/+gitKs3w7/wAizpv/AF6x/wDoIrSrjn8TOiPwoKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK57V/+Rosv+vSX/0JK6GsfVtJu7zUbe7sriGJoonjIljLAhiD2I/u1rSaUtSJpuOhHRUf9k61/wA/lj/4Dv8A/FUf2TrX/P5Y/wDgO/8A8VW/NHuY8suxJRUf9k61/wA/lj/4Dv8A/FUf2TrX/P5Y/wDgO/8A8VRzR7hyy7ElFR/2TrX/AD+WP/gO/wD8VXPeNNfm8C6GNT1m/shE0qxqiwNuYkgcDdzjOaOaPcOWXY6Wiq1rZateWkVzBfWDRyoHUiB+Qf8AgVS/2TrX/P5Y/wDgO/8A8VRzR7hyy7ElFR/2TrX/AD+WP/gO/wD8VR/ZOtf8/lj/AOA7/wDxVHNHuHLLsSVHP/x7yf7p/lR/ZOtf8/lj/wCA7/8AxVI+j606MpvbHDDH+of/AOKp80O4csuxf8O/8izpv/XrH/6CK0qrabaGw0u1tGcOYIljLAYzgYzVmuSTvJs6I6JBRRRUjCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACvn349eF/FXjLWEEUS2mh6cY445JZOJ5ZCBuAGem7HPpX0FXKfEj/kTz/192/8A6NWgDL+EFn4i0bwj/Yfim2KS2DbLecMCssRHGO/HPX1rv6jg/wCPaL/cH8qkoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACuU+JH/Inn/r7t/8A0atdXXKfEj/kTz/192//AKNWgDp4P+PaL/cH8qkqOD/j2i/3B/KpKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKjuRIbWUQf6zYdn1xxQBx+o/FXw3p+pXFkrXl5JanFw9nbmVIT6MR0rWfxpog8HP4oiuvP0tI/MMsI3nHToO/tXE/BW40+D4e3sV1JDHcx3s/2xZSAwbjJOfauU0tSfgf47ltxjT5L24ayx90x+aentQB6RL8W/DEMUNw73f2KYqBfC3P2dS3Yv0B55rpNa8Q6Z4f0dtU1S6SG1XGHJ+8T0A9Sa8+8W2kEP7Nc8cUSqi6SGUAdDszn86wPiKL2Twf4CmS5W3txcxeZPLH5iRuUbazL3FAHomkfE3w/q+qwaepu7K4uRmBb23MPnf7uetdfXkGv+F9W1e+0UeIfHGmv5N7HParBYqju4YEAEMeuK9D8SWGvXunwx+HNVi064Vvnllg8wMMemRQBuV5LoXxNNh4q8UWniC7urwW16I7O1t4BJIieWhOFUZIyTya7/wxY69YWcqeJdUi1KdnykkUHlBV9MZNcN8LrWE/EHx1cGNTKb9ELEc4EaHH6mgDvPDnirSfFWkHUtHuPMgUlX3Da0ZHUMOxrnrr4veF7a4uEV7y4htXKT3VvbF4YyOuXHAxXLeFY5gfipBYAh1vZREq9uH6Vp/DC70SL4H2y3EtukUdvKt2khAIbLbwwPfOaAO11DxRY2fhY69b+bfWhj3xm0jMpcY4IA7Vxngb4xQeJtNtGvtK1FLq5lZA9vYuYQN2B8+T26074QRXCfB3FyrbGM7QBh/yzx8v9aT4F3VrH8JtNSSeFZFkkUqzgEHeeKAPTKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiimSyxwQtLPIscaDLO7YCj1JNAD6Kp2ur6bfTeVZaha3EgGdkM6uceuAauUAFFFFABRRRQAUUUUAFFFFABXKfEj/kTz/192//AKNWurrA8aaXdax4dNrYoHl+0QvgnHCyAn9BQBtwf8e0X+4P5VJTIlKwop6hQD+VPoAKKKKACiiigAooooAKKKKACimTTxW0LTXEqRRIMs8jBVUe5NV7TVtOv5DHY39rcuoyVhmVyB64BoAt0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAcnqnwy8K6vqUl9d6cyzzcym3uJIVk/3lRgG/Gtabwvo8/hl/D7WMaaW8flG3iyg2/UYNa1FAGXe+G9L1Dw02gXduX01ofIMO9hlMYxkHPT3pbjw7pV5oI0W6s45rBUCCGT5sAdOTzn3rTooA5bR/hv4X0PUI72x09mni/1TXFxJN5f+6HYhfwrqaKKACsvSvDel6LfX95ptv5U+oSiW5bezb2wBnBPHAHStSqkuqWUOqQadLcot5cIzxQk/M6r1I+lAFbS/Dml6NeajdafbeVNqc3n3bF2bzH55wTx1PArDu/hX4Qvb6W7m0tlaZ98kcVxJHE59TGrBT+IrsKKAIoLWC1tEtbeFIoEXYsaLhQPTFcpH8K/CUOqG/g0+WGUy+dtiu5Vj35znYG29fauwooAAAFAHQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFcp8Uf+SUeJv8AsGzf+gmurrlPij/ySjxN/wBg2b/0E0AUfhf4Y0TTvB2janY6ZbQXs1igknSMB2yATk/hXL65qK+J/HOuxa5r9zo3h3QRFETb3Bg8yV1U5ZwQerYxmu9+Hn/JOdB/68o/5V57rVrpXhnx34ij8a6a934c18RTxyeWXQSIqgq2OnK5FAGx4C1W80nx/qPhC41KXVLL7JHf2FxO29xGxYEFup+6OtenV5Z4Bs2174kaj4utrKSy0mOyj0+wEi7TIqliWx2HIrpLrxzJH8TrbwhY6Wbktbm4urrztot1xx8u05zkdx1oA6+iiigDgPiprmpWNvoui6NctaXOuXy2huUALRJkbiM98ZrjtTuIPA4tNf8ADHim91aG3vks9Ttri7M6tvzzyTtII7Yrrfizpd9JHoOv6dbPdNod+tzNDGMu8WQGx7gZNcZrB8P+NIrXw74B0iSBr2/ju9Sn8kosYXOd2T1yaAPdqK5vx54wTwR4Wl1Y2hvZhIkcNsr7DK7MFwDg4656dq2tMuZr3S7a5urf7NNNGHaHfu2EjOM4GaALVR3BmFrL9lCmbYfLD/d3Y4z7ZqSo550treSeY4jjUuxx0AGTQB474tTxhY+LPCl1rmuKqXGpeWLLT8xxbcc7j1bt1JFezV4Z8RPiL4d1fW/DElhcyyrYaj5twRC3yLjGa9d0HxPpfiXTXv8ASZzLboSGdlK4x9aANaiuQ8BeOZfHC6lOulGzs7O6a2imabf55XqQNowPxNdfQAUUVjeLvEI8K+Fb3WmtXuxaR7/JjOGf2HBoA2aKzvD+rDXvDmn6qIWtxe26TiJzym5QcH6ZrI8YeNF8J32i2zafLd/2rdfZw0Zx5XT5jwfWgDqKKAcgGuXk8arH8SIfCX9nylpbcz/a8/IMAnGMe3rQB0N9Y22pWMtnfwJcW0y7ZIpFyrD0Iry3w5oum6F+0RqVrpFnDZwf2KreXCgUZ3pzgV6zXmll/wAnJ6l/2BF/9DjoA9LooooAKKKKACiiigAooooAKKKKACiiigAooooAK5f4jWWpXngXUDoV5PZ39vH58MkEhQkodxXjqCARj3rqKRlDKVYZBGCD3oA8u134iSv8EbfV9Ldv7U1GMWluqH5xOcpn6hhmqvjjX9T8DeB/DujHVLn7bfSJBc6iweaUKFJdhjLFuB0qLQ/hZrFj8RFa8aFvC9jeyahZJ5mX811Xgr6BlJ/Guw+Ing648VabZTaVMkGp6Zcrc2zSDKsQCCp9iDQB5w3ikaDrmjzeFNd8Q6yJrpIb611C2uWUxsQC4Mi4UjJP4V1Ot6hqni34pf8ACJ2WqXOladZ2K3VxJasUlldiQF3DkAbT+dallH481HVLIajY6XpFpC4a5aFxM04HZf7oP41F4i8Ka5Z+Po/F/hEW9xcva/ZbqyuW2LIoOQwbsRk+tAGn4b8Na94e16dZNdk1PRJI/wB3HeyNJPHJx/Ec5HXvXBeJfCYv/wBoLTLc6zq0C3WnTTbob2RDERxtQg/KDjkCu68OWfjC58Rz6r4ouIbK08ry4NLtmDqDx87P1J69MdaoeMfDniE+O9J8V+F4bW8ns7eS2ltrmTywyseob15oA3ltx4L8GXjreXl/9liknEt7O00hOMgbmJOK4Hw1oOv+MvBA8T3virVLXUr1Hnt4ra4ZIIRk7F2A4IwBnIr0Cwh1fW/D15a+KrO3s5LhGi8u3k3gKQRnPrXE6RoPxC8LeGZPC+k2+nXlpGHjtNQlm2vGjEkbk7kZ9ulAE+leKdR8S/Am/wBTluHg1GGCSJ7iFtjb0x8wI6VU8CeH9d8ZeEdJ13xD4m1SCWWGN44LO5aNSoAwWwRuLDk59a6Ow8DSaL8KZ/DNjIs93JbuGkY7Q8jdT7Vr+BdIutA8CaNpWohVurO0jhlCNuAZVAOD3oA3Y08uJU3FtqgZJyTTqKKACiiigAooooAKKKKACiiigAooooAKKKKACsXxhos3iPwXq+jWskcU19aSQI8mdqlhgE45xW1RQBxngXR/GGg2lrpmvT6RLp9pbCKM2hk8wkYwTuUDGM12dFFABWHpXha20zxNquu+a095qW0MzgDy0UABB7cCtyigAooooAKKKKAMHxJ4UtfE91pT38ziHTboXQgAG2VgpC5+hIP1Fb1FFABRRRQAVXv7U32m3NoJWhM8TR+YoyU3AjI+masUUAZXhrw/a+GPD9tpNhkxQA/ORguSSST7kmtWiigAooooAKKKKACiiigCpqgvzpc40cwC+2fuTcZ8vd/tYBOK4nwx4Q8T2/xIu/FPie60tzPYi0WKxLnBDKcnco/u16DRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB//2Q==)\r\n",
        "\r\n",
        "However, instead of the 'one-hot-vector', we use the 'padded word2idx Embedding' variant as input here.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuqqLkF283rd"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\r\n",
        "if is_cuda:\r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0cpTIOUnCb1"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class Sentiment_RNN(nn.Module):\r\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        ######################################\r\n",
        "        ######### YOUR CODE HERE #############\r\n",
        "        ######################################\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.embedding_dim = embedding_dim\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.output_dim = output_dim\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\r\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True) \r\n",
        "        self.dropout = nn.Dropout(0.8)   \r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "\r\n",
        "        \r\n",
        "    def forward(self, text):\r\n",
        "\r\n",
        "        ######################################\r\n",
        "        ######### YOUR CODE HERE #############\r\n",
        "        ######################################\r\n",
        "        \r\n",
        "        embedded = self.embedding(text)\r\n",
        "\r\n",
        "        output, hidden = self.rnn(embedded)\r\n",
        "\r\n",
        "        hidden = self.dropout(hidden)\r\n",
        "\r\n",
        "        op1 = self.fc(hidden.squeeze(0))\r\n",
        "        \r\n",
        "        return op1\r\n",
        "        \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvjH-H_jNHev"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Initialize your model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOzUpaEWND76"
      },
      "source": [
        "def init_model(vocab_size):\r\n",
        "\r\n",
        "  input_dim = vocab_size + 1  #Note that since we have added a zero padding, we need to add +1 here.\r\n",
        "  output_dim = 1\r\n",
        "  embedding_dim = 100\r\n",
        "  hidden_dim = 256\r\n",
        "\r\n",
        "  model = Sentiment_RNN(input_dim, embedding_dim, hidden_dim, output_dim)\r\n",
        "  model.to(device)\r\n",
        "\r\n",
        "  criterion = nn.BCEWithLogitsLoss()\r\n",
        "  #optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\r\n",
        "\r\n",
        "  return model, criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtDL-Cjh72Ti"
      },
      "source": [
        "*   Define an accuracy function that will round off the predictions to the nearest integer and checks the accuracy by comparing it with the true label.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_S8NsBM71Sc"
      },
      "source": [
        "def binary_accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns the accuracy activating the prediction using the 'sigmoid' function and tounding off to the closest integer. \r\n",
        "    Compare the rounded_prediction with the true label.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    ######################################\r\n",
        "    ######### YOUR CODE HERE #############\r\n",
        "    ######################################\r\n",
        "    \r\n",
        "    #sig = nn.Sigmoid()\r\n",
        "    preds = torch.sigmoid(preds)\r\n",
        "    preds = torch.round(preds)\r\n",
        "    acc = torch.sum(torch.eq(preds,y))/preds.nelement()\r\n",
        "\r\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Rq2aqSPtxY"
      },
      "source": [
        "# Train your model\r\n",
        "\r\n",
        "*   Compute the loss and binary accuracy.\r\n",
        "*   Average the total loss and accuracy over the 'train_loader' size. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4QrvLkUN6yR"
      },
      "source": [
        "def train_model(model, criterion, optimizer, train_loader):\r\n",
        "  \r\n",
        "  model.train()\r\n",
        "\r\n",
        "  epoch_loss = 0\r\n",
        "  epoch_acc = 0\r\n",
        "\r\n",
        "  for inputs, labels in train_loader:\r\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "    ######################################\r\n",
        "    ######### YOUR CODE HERE #############\r\n",
        "    ######################################\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    predictions = (model(inputs)).squeeze(1)  # Squeeze the model o/p to make predictions shape equals labels shape\r\n",
        "    loss = criterion(predictions.type(torch.FloatTensor), labels.type(torch.FloatTensor))\r\n",
        "\r\n",
        "    acc = binary_accuracy(predictions, labels)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    epoch_acc += acc.item()\r\n",
        "\r\n",
        "  train_loss =  epoch_loss / len(train_loader)\r\n",
        "  train_acc = epoch_acc / len(train_loader)  \r\n",
        "  return train_loss, train_acc\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kzj3k5UPxGl"
      },
      "source": [
        "# Evaluate your model \r\n",
        "\r\n",
        "*   Similar to training process but no back propogation or optimization.\r\n",
        "*   Average the total loss and accuracy over the 'loader' size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMenitQoQAdk"
      },
      "source": [
        "def evaluate_model(model, criterion, loader):\r\n",
        "  epoch_loss = 0\r\n",
        "  epoch_acc = 0\r\n",
        "    \r\n",
        "  model.eval()\r\n",
        "    \r\n",
        "  with torch.no_grad():\r\n",
        "    \r\n",
        "    for inputs, labels in loader:\r\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "      ######################################\r\n",
        "      ######### YOUR CODE HERE #############\r\n",
        "      ######################################\r\n",
        "\r\n",
        "      predictions = (model(inputs)).squeeze(1)  # Squeeze the model o/p to make predictions shape equals labels shape\r\n",
        "      loss = criterion(predictions.type(torch.FloatTensor), labels.type(torch.FloatTensor))\r\n",
        "\r\n",
        "      acc = binary_accuracy(predictions, labels)\r\n",
        "\r\n",
        "      epoch_loss += loss.item()\r\n",
        "      epoch_acc += acc.item()\r\n",
        "        \r\n",
        "  loss =  epoch_loss / len(loader)\r\n",
        "  #print(f'epoch loss---------{loss}')\r\n",
        "  acc = epoch_acc / len(loader) \r\n",
        "  #print(f'epoch acc---------{acc}') \r\n",
        "  return loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg11Pko9QGcK"
      },
      "source": [
        "# Run your sentiment analysis engine\r\n",
        "\r\n",
        "*   Run the below code after implementing all the functions to find the accuracy of the test set using the model.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IrrnVSCQNfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b035e72-066c-48d3-95dc-4bd7ac4852fb"
      },
      "source": [
        "batch_size = 20\r\n",
        "NUM_EPOCHS = 5\r\n",
        "\r\n",
        "train_x, train_y = read_file(\"SJ_Unsupervised_NLP_data.txt\")\r\n",
        "tokenized_preprocessed_reviews = preprocess_text(train_x)\r\n",
        "#tokenized_preprocessed_reviews = [preprocess_text(x) for x in train_x]\r\n",
        "words_list, vocab_size = count_words(tokenized_preprocessed_reviews)\r\n",
        "vocab_to_int, emdedded_vector_list = word2idx_embedding(words_list, tokenized_preprocessed_reviews)\r\n",
        "final_data = pad_text(emdedded_vector_list)\r\n",
        "final_labels = np.array(train_y)\r\n",
        "\r\n",
        "# #Building, training and evaluating your model\r\n",
        "tr_loader, v_loader, t_loader = get_loaders(final_data, final_labels, batch_size)\r\n",
        "model, criterion, optimizer = init_model(vocab_size)\r\n",
        "\r\n",
        "for epoch in range(NUM_EPOCHS):\r\n",
        "\r\n",
        "  train_loss, train_acc = train_model(model, criterion, optimizer, tr_loader)\r\n",
        "  valid_loss, valid_acc = evaluate_model(model, criterion, v_loader)\r\n",
        "\r\n",
        "  print(f'Epoch: {epoch+1:02}')\r\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\r\n",
        "\r\n",
        "# #Evaluate the test accuracy\r\n",
        "test_loss, test_acc = evaluate_model(model, criterion, t_loader)\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\r\n",
        "test_acc_using_RNN = test_acc*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 10 more frequently occuring words are:  ['phone', 'great', 'good', 'product', 'quality', 'headset', 'works', 'battery', 'sound', 'use']\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.731 | Train Acc: 49.38%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 46.00%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.704 | Train Acc: 51.00%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.706 | Train Acc: 52.13%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.712 | Train Acc: 49.75%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.721 | Train Acc: 48.00%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Test Loss: 0.697 | Test Acc: 42.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9f72EwsRixT"
      },
      "source": [
        "# Task 2: Replace 'word2idx' with the pre-trained glove embedding\r\n",
        "\r\n",
        "*   Let us build our own Glove embeding vector file using the Word2Vec function from gensim.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Favc2RUOpl"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpcQOpFgqlQL"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Pre-process the sentence as before by calling the preprocess_text function'\r\n",
        "*   Call the Word2Vec function to create the custom glove embedding using Skip-gram method and set the embedding dimension as 100. Also set the minimum count of the word to be 1. \r\n",
        "*   Save the file.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM5SnP6LdM7j"
      },
      "source": [
        "def preprocess_and_save_glove(data):\r\n",
        "\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  preprocessed_data = preprocess_text(data)\r\n",
        "  model_w2v = Word2Vec(sentences=preprocessed_data, min_count=1, size=100, sg=1)\r\n",
        "  model_w2v.wv.save_word2vec_format('custom_glove_100d.txt')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edq4DNZQr4Fw"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Let us read this 'glove' vector and create a word_to_vector map.\r\n",
        "\r\n",
        "For example, an entry in the map looks like:\r\n",
        "\r\n",
        "'small': array([-0.00258583, -0.0002086 ,  0.00410562, -0.0002388,....])\r\n",
        "\r\n",
        "where 'small' is a word in the glove vector file with it's corresponding embedding vector of 100 dimensions.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "644-SDZvsKOY"
      },
      "source": [
        "def read_glove_vector(glove_vec):\r\n",
        "  with open(glove_vec, 'r', encoding='UTF-8') as f:\r\n",
        "    words = set()\r\n",
        "    word_to_vec_map = {}\r\n",
        "    for line in f:\r\n",
        "      w_line = line.split()\r\n",
        "      curr_word = w_line[0]\r\n",
        "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\r\n",
        "\r\n",
        "  return word_to_vec_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63mjdVKnwvrr"
      },
      "source": [
        "\r\n",
        "*   Create an embedding matrix of size (vocab_size * embedding_size) and fill the corresponding embedding values from the word_to_vec map for each word of the vocabulary.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuKfcumbwT-s"
      },
      "source": [
        "def create_embedding_matrix(vocab_size, embed_size, word_to_vec_map, vocab_to_int):\r\n",
        "  emb_matrix = np.zeros((vocab_size+1, embed_size))\r\n",
        "\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  # Filling the embedding values in the matrix for corresponding words\r\n",
        "  for i in vocab_to_int:\r\n",
        "    emb_matrix[vocab_to_int[i]] = word_to_vec_map[i]\r\n",
        "    \r\n",
        "  return torch.from_numpy(emb_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_TbaZn_sTJk"
      },
      "source": [
        "embed_size = 100\r\n",
        "\r\n",
        "preprocess_and_save_glove(train_x)\r\n",
        "word_to_vec_map = read_glove_vector('custom_glove_100d.txt')\r\n",
        "embedding_matrix = create_embedding_matrix(vocab_size, embed_size, word_to_vec_map, vocab_to_int) #vocab_size and vocab_to_int got from the previous task"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC7USV8g5XKn"
      },
      "source": [
        "# Task 3: Replacing RNNs with LSTMs\r\n",
        "\r\n",
        "*   After replacing the 'word2idx' embedding to the 'glove' embedding, we shall replace our simple RNN to an LSTM and see if it impacts the accuracy\r\n",
        "*   Only the model building code changes. However, the training and valiation function remains the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcVAy7vd5UBD"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class Sentiment_LSTM(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "        # embedding and LSTM layers\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \r\n",
        "                            dropout=dropout, batch_first=True)\r\n",
        "        \r\n",
        "        # dropout layer\r\n",
        "        self.dropout = nn.Dropout(0.3)\r\n",
        "\r\n",
        "        # linear and sigmoid layers\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, x,hidden):\r\n",
        "        \"\"\"\r\n",
        "        Perform a forward pass of our model on some input and hidden state.\r\n",
        "        \"\"\"\r\n",
        "        batch_size = x.size(0)\r\n",
        "\r\n",
        "        # embeddings and lstm_out\r\n",
        "        embeds = self.embedding(x)\r\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\r\n",
        "        \r\n",
        "    \r\n",
        "        # stack up lstm outputs\r\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\r\n",
        "        \r\n",
        "        # dropout and fully-connected layer\r\n",
        "        out = self.dropout(lstm_out)\r\n",
        "        out = self.fc(out)\r\n",
        "\r\n",
        "        out = out.view(batch_size, -1)\r\n",
        "        out = out[:, -1]\r\n",
        "        \r\n",
        "        return out, hidden\r\n",
        "\r\n",
        "\r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        ''' Initializes hidden state '''\r\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\r\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        \r\n",
        "        if (device.type == 'cuda'):\r\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\r\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\r\n",
        "        else:\r\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\r\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\r\n",
        "        \r\n",
        "        return hidden\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3ygVeY39o37"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   While initiating our model, we have to perform one additional step of copying the weights of the embedding layer computed in the previous step using the glove embedding (which is the embedding matrix). Hence, we don't need to additionally learn the embedding weights since it is now replaced by the pre-trained embeddings.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJkBP0Zg74Bc"
      },
      "source": [
        "def init_LSTM_model(vocab_size):\r\n",
        "\r\n",
        "  input_dim = vocab_size + 1  #Note that since we have added a zero padding, we need to add +1 here.\r\n",
        "  output_dim = 1\r\n",
        "  embedding_dim = 100\r\n",
        "  hidden_dim = 256\r\n",
        "  n_layers = 2\r\n",
        "  bidirectional = True\r\n",
        "  dropout = 0.5\r\n",
        "\r\n",
        "  model = Sentiment_LSTM(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout)\r\n",
        "  model.to(device)\r\n",
        "\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "  model.embedding.from_pretrained(embedding_matrix)\r\n",
        "\r\n",
        "  criterion = nn.BCEWithLogitsLoss()\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n",
        "\r\n",
        "  return model, criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuWWk-OY9HSs"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Run the below code after implementing all the functions to find the accuracy of the test set using the model.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GZbjkusDt1p"
      },
      "source": [
        "def train_model_lstm(model, criterion, optimizer, train_loader, h):\r\n",
        "  \r\n",
        "  model.train()\r\n",
        "\r\n",
        "  epoch_loss = 0\r\n",
        "  epoch_acc = 0\r\n",
        "\r\n",
        "  for inputs, labels in train_loader:\r\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "    ######################################\r\n",
        "    ######### YOUR CODE HERE #############\r\n",
        "    ######################################\r\n",
        "\r\n",
        "    # Creating new variables for the hidden state, otherwise\r\n",
        "    # we'd backprop through the entire training history\r\n",
        "    h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    output, h = model(inputs, h)  # Squeeze the model o/p to make predictions shape equals labels shape\r\n",
        "   \r\n",
        "    loss = criterion(output.squeeze(), labels.float())\r\n",
        "\r\n",
        "    acc = binary_accuracy(output, labels)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    epoch_acc += acc.item()\r\n",
        "\r\n",
        "  train_loss =  epoch_loss / len(train_loader)\r\n",
        "  train_acc = epoch_acc / len(train_loader)  \r\n",
        "  return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPbJGpZHJZyy"
      },
      "source": [
        "def evaluate_model_lstm(model, criterion, loader, h):\r\n",
        "  epoch_loss = 0\r\n",
        "  epoch_acc = 0\r\n",
        "    \r\n",
        "  model.eval()\r\n",
        "    \r\n",
        "  with torch.no_grad():\r\n",
        "    \r\n",
        "    for inputs, labels in loader:\r\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "      ######################################\r\n",
        "      ######### YOUR CODE HERE #############\r\n",
        "      ######################################\r\n",
        "\r\n",
        "      h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "      output, h = model(inputs, h)  # Squeeze the model o/p to make predictions shape equals labels shape\r\n",
        "      loss = criterion(output.squeeze(), labels.float())\r\n",
        "\r\n",
        "      acc = binary_accuracy(output, labels)\r\n",
        "\r\n",
        "      epoch_loss += loss.item()\r\n",
        "      epoch_acc += acc.item()\r\n",
        "        \r\n",
        "  loss =  epoch_loss / len(loader)\r\n",
        "  acc = epoch_acc / len(loader) \r\n",
        " \r\n",
        "  return loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6roedh9-QUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440a284f-b0c7-4265-a671-c23168995350"
      },
      "source": [
        "NUM_EPOCHS = 20\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model, criterion, optimizer = init_LSTM_model(vocab_size)\r\n",
        "\r\n",
        "for epoch in range(NUM_EPOCHS):\r\n",
        "\r\n",
        "  h = model.init_hidden(batch_size)\r\n",
        "\r\n",
        "  train_loss, train_acc = train_model_lstm(model, criterion, optimizer, tr_loader, h)\r\n",
        "  valid_loss, valid_acc = evaluate_model_lstm(model, criterion, v_loader, h)\r\n",
        "\r\n",
        "  print(f'Epoch: {epoch+1:02}')\r\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\r\n",
        "\r\n",
        "# #Evaluate the test accuracy\r\n",
        "test_loss, test_acc = evaluate_model_lstm(model, criterion, t_loader, h)\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\r\n",
        "test_acc_using_LSTM = test_acc*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.699 | Train Acc: 52.13%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 49.00%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.694 | Train Acc: 51.13%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.00%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.695 | Train Acc: 50.50%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 51.00%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.696 | Train Acc: 48.75%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 51.00%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.695 | Train Acc: 48.25%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 49.00%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.694 | Train Acc: 51.00%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.38%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.693 | Train Acc: 51.00%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.00%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.00%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.694 | Train Acc: 48.25%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.00%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.63%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.00%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.672 | Train Acc: 60.75%\n",
            "\t Val. Loss: 0.600 |  Val. Acc: 74.00%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.585 | Train Acc: 73.13%\n",
            "\t Val. Loss: 0.588 |  Val. Acc: 71.00%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.489 | Train Acc: 79.50%\n",
            "\t Val. Loss: 0.523 |  Val. Acc: 76.00%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.371 | Train Acc: 87.25%\n",
            "\t Val. Loss: 0.555 |  Val. Acc: 76.00%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.313 | Train Acc: 88.75%\n",
            "\t Val. Loss: 0.641 |  Val. Acc: 74.00%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.230 | Train Acc: 92.88%\n",
            "\t Val. Loss: 0.659 |  Val. Acc: 77.00%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.156 | Train Acc: 95.38%\n",
            "\t Val. Loss: 0.637 |  Val. Acc: 75.00%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.12%\n",
            "\t Val. Loss: 0.781 |  Val. Acc: 79.00%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.110 | Train Acc: 96.75%\n",
            "\t Val. Loss: 0.714 |  Val. Acc: 80.00%\n",
            "Test Loss: 1.068 | Test Acc: 65.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiFyqmLULoO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4073ea81-b02c-4348-e2f7-493520b68930"
      },
      "source": [
        "print(f'The test accuracy using the RNNs is: {test_acc_using_RNN}%')\r\n",
        "print(f'The test accuracy using the LSTMs is: {test_acc_using_LSTM}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy using the RNNs is: 42.00000047683716%\n",
            "The test accuracy using the LSTMs is: 65.00000238418579%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LlBAiyQCIsu"
      },
      "source": [
        "# Task 4: Optional task for learning\r\n",
        "\r\n",
        "*   Now that you have a good understanding about pre-processing the text, perform a much stringent pre-processing removing the nouns, verbs and adjectives, and find out the accuracy for each part-of-speech removal\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0kU52ouCuz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c60d83-2ba1-466f-f642-747d7f2c367c"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlNLncZxOy6Y"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Also, create a map with 'noun', 'verb' and 'adjective' as the key and it's corresponding reviews as the values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7MbG2Z1DOPD"
      },
      "source": [
        "def remove_pos_from_reviews(reviews):\r\n",
        "\r\n",
        "  all_reviews_without_nouns = []\r\n",
        "  all_reviews_without_verbs = []\r\n",
        "  all_reviews_without_adjectives = []\r\n",
        "\r\n",
        "  all_review_maps = {}\r\n",
        "  ######################################\r\n",
        "  ######### YOUR CODE HERE #############\r\n",
        "  ######################################\r\n",
        "\r\n",
        "  return all_review_maps #Containing all the reviews of the noun, verb and adjective void text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3kiTPoQPoNd"
      },
      "source": [
        "# Build the model engine\r\n",
        "\r\n",
        "*   Looking at the previous sections (Refer to : Run your sentiment analysis engine), build the engine that will create the dataloaders, build-train-evaluate the models.\r\n",
        "*   Loop through each element in 'all_review_maps' to calculate the accuracy of each Part-of-speech removal. Print the accuracies for Noun, Verb, Adjectives.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwhRDofKEZKo"
      },
      "source": [
        "batch_size = 20\r\n",
        "NUM_EPOCHS = 5\r\n",
        "\r\n",
        "######################################\r\n",
        "######### YOUR CODE HERE #############\r\n",
        "######################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}